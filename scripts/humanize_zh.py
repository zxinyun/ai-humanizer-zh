#!/usr/bin/env python3
"""
AI-Humanizer-ZH - ä¸­æ–‡AIæ–‡æœ¬äººç±»åŒ–å·¥å…·
ä¸“é—¨å»é™¤AIç”Ÿæˆæ–‡æœ¬çš„æœºæ¢°æ„Ÿå’Œå…¬å¼åŒ–è¡¨è¾¾ï¼Œä½¿å…¶æ›´è‡ªç„¶æµç•…ã€æ›´åƒäººç±»å†™ä½œé£æ ¼
åŸºäºHumanizer-zhé¡¹ç›®ç†å¿µä¼˜åŒ–ï¼Œæ”¯æŒå¤šç§é£æ ¼è½¬æ¢å’Œæ‰¹é‡å¤„ç†
"""

import argparse
import random
import re
import os
import sys
from typing import List, Dict, Set

# å¸¸è§AIè¯æ±‡è­¦ç¤ºåˆ—è¡¨
AI_WORDS = {
    'æ­¤å¤–', 'è‡³å…³é‡è¦', 'æ·±å…¥æ¢è®¨', 'å¼ºè°ƒ', 'æŒä¹…çš„', 'å¢å¼º', 'åŸ¹å…»', 'è·å¾—',
    'çªå‡º', 'ç›¸äº’ä½œç”¨', 'å¤æ‚', 'å¤æ‚æ€§', 'æ ¼å±€', 'å…³é”®æ€§çš„', 'å±•ç¤º', 'ç»‡é”¦',
    'è¯æ˜', 'å¼ºè°ƒ', 'å®è´µçš„', 'å……æ»¡æ´»åŠ›çš„', 'æ— ç¼', 'ç›´è§‚', 'å¼ºå¤§', 'é©å‘½æ€§',
    'åˆ›æ–°', 'å‘å±•', 'è¶‹åŠ¿', 'æœªæ¥', 'æŒ‘æˆ˜', 'æœºé‡', 'é‡è¦', 'æ ¸å¿ƒ', 'å…³é”®',
    'åŸºç¡€', 'å…³é”®', 'ä¸»è¦', 'é‡ç‚¹', 'æ ¸å¿ƒ', 'é¦–è¦', 'é‡è¦', 'å¿…è¦', 'ä¸å¯æˆ–ç¼º'
}

# ä¸­æ–‡æ›¿æ¢è§„åˆ™ - åŸºäºHumanizer-zhçš„ä¼˜åŒ–
ZH_REPLACEMENTS = {
    # æ­£å¼è¡¨è¾¾è½¬å£è¯­åŒ–
    'ç»¼ä¸Šæ‰€è¿°': ['æ€»çš„æ¥è¯´', 'æ€»è€Œè¨€ä¹‹', 'æ•´ä½“æ¥è¯´', 'ç›´ç™½ç‚¹è¯´'],
    'ç”±æ­¤å¯è§': ['çœ‹å¾—å‡ºæ¥', 'è¿™è¯´æ˜', 'æ˜¾è€Œæ˜“è§', 'å¦‚æ­¤è¯´æ¥'],
    'ä¼—æ‰€å‘¨çŸ¥': ['å¤§å®¶éƒ½çŸ¥é“', 'ä¼—æ‰€å‘¨çŸ¥', 'è°éƒ½æ˜ç™½', 'ä¸ç”¨è¯´'],
    'æ˜¾è€Œæ˜“è§': ['å¾ˆæ˜æ˜¾', 'æ˜æ‘†ç€', 'ä¸€çœ¼å°±èƒ½çœ‹å‡ºæ¥', 'æ˜¾ç„¶'],
    'ä¸å¯æˆ–ç¼º': ['å°‘ä¸äº†', 'å¾ˆé‡è¦', 'ä¸å¯å°‘', 'ç¼ºä¸äº†'],
    'è‡³å…³é‡è¦': ['éå¸¸é‡è¦', 'ç‰¹åˆ«å…³é”®', 'æå…¶é‡è¦', 'é‡ä¸­ä¹‹é‡'],
    'ä¸æ­¤åŒæ—¶': ['ä¸æ­¤åŒæ—¶', 'åŒæ—¶', 'ä¹Ÿ', 'å¹¶ä¸”'],
    'å€¼å¾—ä¸€æçš„æ˜¯': ['å€¼å¾—ä¸€æçš„æ˜¯', 'è¦ç‰¹åˆ«æä¸€ä¸‹', 'æœ‰ä¸€ç‚¹è¦æ³¨æ„', 'è¿™é‡Œè¦æä¸€ä¸‹'],
    'æ¢å¥è¯è¯´': ['æ¢å¥è¯è¯´', 'ä¹Ÿå°±æ˜¯è¯´', 'è¯´ç™½äº†', 'ç®€å•ç‚¹è¯´'],
    'ä¾‹å¦‚': ['æ¯”å¦‚', 'ä¾‹å¦‚', 'æ¯”æ–¹è¯´', 'ä¸¾ä¸ªä¾‹å­'],
    'å› æ­¤': ['æ‰€ä»¥', 'å› æ­¤', 'æ•…è€Œ', 'è¿™å°±å¯¼è‡´'],
    'å› ä¸º': ['å› ä¸º', 'ç”±äº', 'é‰´äº', 'å°±å› ä¸º'],
    'ä½†æ˜¯': ['ä¸è¿‡', 'ä½†æ˜¯', 'å¯æ˜¯', 'ç„¶è€Œ'],
    'è€Œä¸”': ['è€Œä¸”', 'å¹¶ä¸”', 'è¿˜', 'ç”šè‡³'],
    'ç„¶è€Œ': ['ç„¶è€Œ', 'ä½†æ˜¯', 'ä¸è¿‡', 'å¯'],
    'é¦–å…ˆ': ['é¦–å…ˆ', 'ç¬¬ä¸€', 'å…ˆ', 'ç¬¬ä¸€æ­¥'],
    'å…¶æ¬¡': ['å…¶æ¬¡', 'ç¬¬äºŒ', 'æ¥ç€', 'ä¸‹ä¸€æ­¥'],
    'æœ€å': ['æœ€å', 'æœ€ç»ˆ', 'è¯´åˆ°åº•', 'æœ€åä¸€æ­¥'],
    
    # AIå¸¸è§å¥—è¯æ›¿æ¢
    'åœ¨æœ¬æ–‡ä¸­': ['åœ¨è¿™ç¯‡æ–‡ç« é‡Œ', 'åœ¨è¿™é‡Œ', 'æœ¬æ–‡ä¸­', 'åœ¨è¿™ç¯‡å†…å®¹é‡Œ'],
    'æˆ‘ä»¬å°†è®¨è®º': ['å’±ä»¬æ¥èŠèŠ', 'æˆ‘ä»¬è¦è®¨è®º', 'è¿™é‡Œè°ˆè°ˆ', 'æˆ‘ä»¬èŠä¸€èŠ'],
    'æœ¬æ–‡æ—¨åœ¨': ['è¿™ç¯‡æ–‡ç« ä¸»è¦æƒ³', 'æœ¬æ–‡ä¸»è¦', 'è¿™ç¯‡æ–‡ç« æ—¨åœ¨', 'æœ¬æ–‡ç›®çš„æ˜¯'],
    'åŸºäºä»¥ä¸Šåˆ†æ': ['æ ¹æ®ä¸Šé¢çš„åˆ†æ', 'åŸºäºä»¥ä¸Šåˆ†æ', 'ä»ä¸Šæ–‡åˆ†ææ¥çœ‹', 'ç»¼åˆä»¥ä¸Šåˆ†æ'],
    'è¿™è¡¨æ˜': ['è¿™è¯´æ˜', 'è¿™è¡¨æ˜', 'è¿™è¡¨ç¤º', 'è¿™æ˜¾ç¤º'],
    'ç ”ç©¶è¡¨æ˜': ['æœ‰ç ”ç©¶æ˜¾ç¤º', 'ç ”ç©¶è¡¨æ˜', 'ç ”ç©¶å‘ç°', 'æ®ç ”ç©¶'],
    'æ•°æ®æ˜¾ç¤º': ['æ•°æ®æ˜¾ç¤º', 'æ•°æ®è¡¨æ˜', 'ç»Ÿè®¡æ˜¾ç¤º', 'æ®æ•°æ®ç»Ÿè®¡'],
    
    # æœºæ¢°æ„Ÿè¯æ±‡æ›¿æ¢
    'è¿›è¡Œ': ['åš', 'è¿›è¡Œ', 'å®æ–½', 'å¼€å±•'],
    'å¼€å±•': ['å±•å¼€', 'å¼€å±•', 'æ¨è¿›', 'æèµ·æ¥'],
    'å®æ–½': ['æ¨è¡Œ', 'å®æ–½', 'æ‰§è¡Œ', 'è½å®'],
    'å®ç°': ['è¾¾åˆ°', 'å®ç°', 'å®Œæˆ', 'è¾¾æˆ'],
    'å®Œæˆ': ['åšå®Œ', 'å®Œæˆ', 'æå®š', 'ç»“æŸ'],
    'æä¾›': ['ç»™', 'æä¾›', 'ç»™äºˆ', 'ä¾›åº”'],
    'è·å¾—': ['å¾—åˆ°', 'è·å¾—', 'æ‹¿åˆ°', 'å–å¾—'],
    'å…·æœ‰': ['æœ‰', 'å…·æœ‰', 'å…·å¤‡', 'æ‹¥æœ‰'],
    'å­˜åœ¨': ['æœ‰', 'å­˜åœ¨', 'å…·å¤‡', 'æœ‰å¾ˆå¤š'],
    'åŒ…å«': ['åŒ…æ‹¬', 'åŒ…å«', 'æ¶µç›–', 'é‡Œé¢æœ‰'],
    'æ¶‰åŠ': ['ç‰µæ‰¯åˆ°', 'æ¶‰åŠ', 'å…³ç³»åˆ°', 'å’Œ...æœ‰å…³'],
    'ä½¿ç”¨': ['ç”¨', 'ä½¿ç”¨', 'è¿ç”¨', 'é‡‡ç”¨'],
    'åˆ©ç”¨': ['å€ŸåŠ©', 'åˆ©ç”¨', 'ä½¿ç”¨', 'å……åˆ†åˆ©ç”¨'],
    'é€šè¿‡': ['é€šè¿‡', 'å€ŸåŠ©', 'ç»ç”±', 'é '],
    'ä½¿å¾—': ['è®©', 'ä½¿å¾—', 'è‡´ä½¿', 'å¯¼è‡´'],
    'å¯¼è‡´': ['å¯¼è‡´', 'é€ æˆ', 'å¼•èµ·', 'ä½¿å¾—'],
    'å¼•èµ·': ['å¼•å‘', 'å¼•èµ·', 'å¯¼è‡´', 'æ‹›æ¥'],
    'äº§ç”Ÿ': ['äº§ç”Ÿ', 'å½¢æˆ', 'å¸¦æ¥', 'å¼•å‘'],
    'å‘ç”Ÿ': ['å‘ç”Ÿ', 'å‡ºç°', 'äº§ç”Ÿ', 'çˆ†å‘'],
    'å‡ºç°': ['å‡ºç°', 'æ˜¾ç°', 'å‘ç”Ÿ', 'å†’å‡ºæ¥'],
    
    # AIä¸‰è¿è¯æ›¿æ¢
    'æ— ç¼ã€ç›´è§‚å’Œå¼ºå¤§': ['ç®€å•å¥½ç”¨', 'åŠŸèƒ½å¼ºå¤§', 'æ˜“äºæ“ä½œ', 'ç”¨æˆ·å‹å¥½'],
    'é«˜æ•ˆã€ç¨³å®šå’Œå¯é ': ['è¿è¡Œç¨³å®š', 'é«˜æ•ˆå¯é ', 'æ€§èƒ½ç¨³å®š', 'è¡¨ç°å‡ºè‰²'],
    'åˆ›æ–°ã€çªç ´å’Œå‘å±•': ['ä¸æ–­åˆ›æ–°', 'æŒç»­å‘å±•', 'çªç ´è¿›æ­¥', 'ç¨³æ­¥å‰è¿›'],
    'æŒ‘æˆ˜ã€æœºé‡å’Œæœªæ¥': ['æœºé‡ä¸æŒ‘æˆ˜', 'æœªæ¥å‘å±•', 'å‰æ™¯å±•æœ›', 'å‘å±•å‰æ™¯']
}

# ä¸­æ–‡å¥å­å¼€å¤´å˜åŒ–
ZH_SENTENCE_STARTERS = [
    'ä½ çŸ¥é“å—ï¼Œ', 'æœ‰æ„æ€çš„æ˜¯ï¼Œ', 'æˆ‘å‘ç°ï¼Œ', 'å…¶å®ï¼Œ', 'è¯´çœŸçš„ï¼Œ',
    'è€å®è¯´ï¼Œ', 'ä¸å¾—ä¸è¯´ï¼Œ', 'å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œ', 'æœ‰è¶£çš„æ˜¯ï¼Œ', 'è®©äººæƒŠè®¶çš„æ˜¯ï¼Œ',
    'ä½ çŒœæ€ä¹ˆç€ï¼Œ', 'æˆ‘è§‰å¾—å§ï¼Œ', 'è¦æˆ‘è¯´ï¼Œ', 'ä¾æˆ‘çœ‹ï¼Œ', 'æ®æˆ‘æ‰€çŸ¥ï¼Œ'
]

# å£è¯­åŒ–å¡«å……è¯
ZH_FILLER_WORDS = ['å‘¢', 'å•Š', 'å§', 'å•¦', 'å˜›', 'å˜¿', 'å—¯', 'å“¦', 'å‘€']

# å¦å®šå¼æ’æ¯”æ£€æµ‹æ¨¡å¼
NEGATIVE_PATTERNS = [
    r'è¿™ä¸ä»…ä»…æ˜¯.*æ›´æ˜¯', r'è¿™ä¸åªæ˜¯.*è€Œæ˜¯', r'ä¸ä»…ä»…æ˜¯.*è€Œä¸”æ˜¯',
    r'ä¸ä»…æ˜¯.*æ›´æ˜¯', r'ä¸åªæ˜¯.*è€Œæ˜¯', r'å¹¶é.*è€Œæ˜¯'
]

def detect_ai_patterns(text: str) -> Dict[str, int]:
    """æ£€æµ‹æ–‡æœ¬ä¸­çš„AIå†™ä½œæ¨¡å¼"""
    patterns = {
        'ai_words': 0,              # AIè¯æ±‡ä½¿ç”¨é¢‘ç‡
        'negative_parallelism': 0,  # å¦å®šå¼æ’æ¯”
        'triple_structure': 0,      # ä¸‰æ®µå¼ç»“æ„
        'excessive_formality': 0,   # è¿‡åº¦æ­£å¼
        'empty_phrase': 0,          # ç©ºæ³›çŸ­è¯­
        'exaggeration': 0,          # è¿‡åº¦å¤¸å¼ 
        'vague_attribution': 0,     # æ¨¡ç³Šå½’å› 
    }
    
    # æ£€æµ‹AIè¯æ±‡
    words = re.findall(r'[\u4e00-\u9fff]+', text)
    for word in words:
        if word in AI_WORDS:
            patterns['ai_words'] += 1
    
    # æ£€æµ‹å¦å®šå¼æ’æ¯”
    for pattern in NEGATIVE_PATTERNS:
        matches = re.findall(pattern, text)
        patterns['negative_parallelism'] += len(matches)
    
    # æ£€æµ‹ä¸‰æ®µå¼ç»“æ„
    triple_matches = re.findall(r'([ï¼Œ,ï¼›;]).*?\1.*?\1', text)
    patterns['triple_structure'] += len(triple_matches)
    
    # æ£€æµ‹ç©ºæ³›çŸ­è¯­
    empty_phrases = ['å‘å±•è¶‹åŠ¿', 'æœªæ¥å±•æœ›', 'æŒ‘æˆ˜ä¸æœºé‡', 'æ ¸å¿ƒç«äº‰åŠ›', 'é‡è¦æ„ä¹‰']
    for phrase in empty_phrases:
        if phrase in text:
            patterns['empty_phrase'] += 1
    
    # æ£€æµ‹è¿‡åº¦å¤¸å¼ 
    exaggeration_phrases = ['é©å‘½æ€§', 'çªç ´æ€§', 'é¢ è¦†æ€§', 'æ— ä¸ä¼¦æ¯”', 'ç‹¬ä¸€æ— äºŒ']
    for phrase in exaggeration_phrases:
        if phrase in text:
            patterns['exaggeration'] += 1
    
    # æ£€æµ‹æ¨¡ç³Šå½’å› 
    vague_phrases = ['ç ”ç©¶è¡¨æ˜', 'æ•°æ®æ˜¾ç¤º', 'ä¸“å®¶è®¤ä¸º', 'æ®æŠ¥é“', 'ä¼—æ‰€å‘¨çŸ¥']
    for phrase in vague_phrases:
        if phrase in text:
            patterns['vague_attribution'] += 1
    
    return patterns

def apply_zh_replacements(text: str, variability: str = 'medium') -> str:
    """åº”ç”¨ä¸­æ–‡æ–‡æœ¬æ›¿æ¢è§„åˆ™"""
    for old, new_list in ZH_REPLACEMENTS.items():
        if old in text:
            # æ ¹æ®variabilityé€‰æ‹©æ›¿æ¢æ–¹å¼
            if variability == 'low':
                # ä½å˜åŒ–ï¼šä½¿ç”¨ç¬¬ä¸€ä¸ªæ›¿æ¢è¯
                text = text.replace(old, new_list[0])
            elif variability == 'medium':
                # ä¸­å˜åŒ–ï¼šéšæœºé€‰æ‹©æ›¿æ¢è¯
                text = text.replace(old, random.choice(new_list))
            else:  # high
                # é«˜å˜åŒ–ï¼šä½¿ç”¨æ›´å£è¯­åŒ–çš„æ›¿æ¢
                text = text.replace(old, random.choice(new_list[-2:]) if len(new_list) >= 2 else new_list[0])
    return text

def reduce_ai_words(text: str, variability: str = 'medium') -> str:
    """å‡å°‘AIè¯æ±‡çš„ä½¿ç”¨"""
    # éšæœºæ›¿æ¢éƒ¨åˆ†AIè¯æ±‡
    words = text.split()
    for i, word in enumerate(words):
        # æ£€æŸ¥æ˜¯å¦åŒ…å«AIè¯æ±‡
        for ai_word in AI_WORDS:
            if ai_word in word and random.random() < 0.3 + {'low': 0, 'medium': 0.2, 'high': 0.4}[variability]:
                # æ ¹æ®ä¸Šä¸‹æ–‡é€‰æ‹©åˆé€‚çš„æ›¿æ¢
                replacements = {
                    'åˆ›æ–°': ['å‡ºæ–°æ‹›', 'ç©æ–°èŠ±æ ·', 'æåˆ›æ–°'],
                    'å‘å±•': ['å˜å¥½', 'è¿›æ­¥', 'å¾€å‰èµ°'],
                    'é‡è¦': ['è¦ç´§', 'å…³é”®', 'æœ‰ç”¨'],
                    'å¤æ‚': ['ç»•äºº', 'éº»çƒ¦', 'ä¸ç®€å•'],
                    'è¶‹åŠ¿': ['é£å‘', 'èµ°å‘', 'åŠ¿å¤´']
                }
                if ai_word in replacements:
                    words[i] = words[i].replace(ai_word, random.choice(replacements[ai_word]))
                break
    
    return ' '.join(words)

def fix_negative_parallelism(text: str) -> str:
    """ä¿®å¤å¦å®šå¼æ’æ¯”ç»“æ„"""
    # æ›¿æ¢å¦å®šå¼æ’æ¯”
    replacements = {
        r'è¿™ä¸ä»…ä»…æ˜¯(.*?)æ›´æ˜¯(.*?)': lambda m: f'è¿™ä¸åªæ˜¯{m.group(1)}ï¼Œæ›´æ˜¯{m.group(2)}',
        r'è¿™ä¸åªæ˜¯(.*?)è€Œæ˜¯(.*?)': lambda m: f'å…¶å®è¿™ä¸æ˜¯{m.group(1)}ï¼Œè€Œæ˜¯{m.group(2)}',
        r'ä¸ä»…ä»…æ˜¯(.*?)è€Œä¸”æ˜¯(.*?)': lambda m: f'è¿™ä¸ä»…æ˜¯{m.group(1)}ï¼Œè€Œä¸”æ˜¯{m.group(2)}',
        r'ä¸ä»…æ˜¯(.*?)æ›´æ˜¯(.*?)': lambda m: f'è¿™ä¸åªæ˜¯{m.group(1)}ï¼Œè¿˜æ˜¯{m.group(2)}',
        r'ä¸åªæ˜¯(.*?)è€Œæ˜¯(.*?)': lambda m: f'å…¶å®å®ƒä¸æ˜¯{m.group(1)}ï¼Œè€Œæ˜¯{m.group(2)}',
        r'å¹¶é(.*?)è€Œæ˜¯(.*?)': lambda m: f'è¿™ä¸æ˜¯{m.group(1)}ï¼Œè€Œæ˜¯{m.group(2)}'
    }
    
    for pattern, replacer in replacements.items():
        matches = re.findall(pattern, text)
        for match in matches:
            text = re.sub(pattern, replacer, text)
    
    return text

def vary_chinese_sentence_lengths(text: str, variability: str = 'medium') -> str:
    """è°ƒæ•´ä¸­æ–‡å¥å­é•¿åº¦å˜åŒ–"""
    # æŒ‰ä¸­æ–‡æ ‡ç‚¹åˆ†å‰²
    sentences = re.split(r'([ã€‚ï¼ï¼Ÿï¼›.!?;])', text)
    
    # é‡æ–°ç»„åˆå¥å­å’Œæ ‡ç‚¹
    combined_sentences = []
    for i in range(0, len(sentences)-1, 2):
        combined_sentences.append(sentences[i] + sentences[i+1])
    
    if variability == 'low':
        return ''.join(combined_sentences)
    
    # æ ¹æ®å¯å˜æ€§è°ƒæ•´å¥å­é•¿åº¦
    varied_sentences = []
    for sentence in combined_sentences:
        sentence = sentence.strip()
        if not sentence:
            continue
            
        # å¤„ç†ä¸­æ–‡é•¿å¥
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', sentence))
        
        # é•¿å¥æ‹†åˆ†ä¸ºçŸ­å¥
        if chinese_chars > 40 and variability in ['medium', 'high']:
            # æŒ‰é€—å·ã€åˆ†å·ç­‰åˆ†å‰²
            clauses = re.split(r'([ï¼Œ,ï¼›;ã€])', sentence)
            if len(clauses) > 3:
                # æ‹†åˆ†æˆå¤šä¸ªå¥å­
                new_sentences = []
                current = ''
                for i, part in enumerate(clauses):
                    current += part
                    chinese_count = len(re.findall(r'[\u4e00-\u9fff]', current))
                    
                    # æ¯15-25ä¸ªæ±‰å­—åˆ†ä¸€ä¸ªå¥å­
                    if chinese_count > 15 and (i + 1) % 2 == 0:
                        new_sentences.append(current.strip() + 'ã€‚')
                        current = ''
                
                if current:
                    new_sentences.append(current.strip() + 'ã€‚')
                varied_sentences.extend(new_sentences)
                continue
        
        # åˆå¹¶è¿‡çŸ­çš„å¥å­
        elif chinese_chars < 10 and variability in ['high'] and varied_sentences:
            # æ£€æŸ¥å‰ä¸€ä¸ªå¥å­
            prev_sentence = varied_sentences[-1]
            prev_chars = len(re.findall(r'[\u4e00-\u9fff]', prev_sentence))
            
            if prev_chars < 30:  # å‰ä¸€ä¸ªå¥å­ä¹Ÿä¸å¤ªé•¿
                varied_sentences[-1] = prev_sentence.rstrip('ã€‚ï¼ï¼Ÿï¼›.!?;') + 'ï¼Œ' + sentence.lstrip('ã€‚ï¼ï¼Ÿï¼›.!?;')
                continue
        
        varied_sentences.append(sentence)
    
    return ''.join(varied_sentences)

def add_chinese_natural_variations(text: str, variability: str = 'medium', style: str = 'casual') -> str:
    """å¢åŠ ä¸­æ–‡è‡ªç„¶çš„è¯­è¨€å˜åŒ–"""
    
    # éšæœºæ·»åŠ å£è¯­åŒ–å¼€å¤´
    sentences = text.split('ã€‚')
    if sentences and len(sentences) > 1:
        for i in range(len(sentences)-1):
            if random.random() < 0.2 and variability in ['medium', 'high'] and style in ['casual', 'creative']:
                if not any(elem in sentences[i] for elem in ZH_SENTENCE_STARTERS):
                    sentences[i] = random.choice(ZH_SENTENCE_STARTERS) + sentences[i]
        text = 'ã€‚'.join(sentences)
    
    # æ·»åŠ å£è¯­åŒ–å¡«å……è¯
    if random.random() < 0.4 and style in ['casual', 'creative']:
        words = re.split(r'([ã€‚ï¼ï¼Ÿï¼›,.!?;])', text)
        for i in range(len(words)-1):
            if random.random() < 0.1 and variability in ['medium', 'high']:
                if re.match(r'[\u4e00-\u9fff]+', words[i]):
                    words[i] += random.choice(ZH_FILLER_WORDS)
        text = ''.join(words)
    
    # æ›¿æ¢è¿‡åº¦ä½¿ç”¨çš„è¯è¯­
    if variability in ['medium', 'high']:
        synonyms = {
            'éå¸¸': ['ç‰¹åˆ«', 'ååˆ†', 'æå…¶', 'è¶…çº§', 'æ ¼å¤–'],
            'å¾ˆå¤š': ['ä¸å°‘', 'è®¸å¤š', 'å¥½å¤š', 'ä¸€å¤§å †', 'æŒºå¤š'],
            'é‡è¦': ['å…³é”®', 'è¦ç´§', 'é‡è¦', 'é‡å¤§', 'ç´§è¦'],
            'æœ‰è¶£': ['æœ‰æ„æ€', 'æœ‰è¶£', 'å¥½ç©', 'é€—ä¹', 'æç¬‘'],
            'ç®€å•': ['å®¹æ˜“', 'ç®€å•', 'è½»æ¾', 'å°èœä¸€ç¢Ÿ', 'å¥½å¼„'],
            'å›°éš¾': ['ä¸å®¹æ˜“', 'å›°éš¾', 'éº»çƒ¦', 'è´¹åŠ²', 'éš¾åŠ'],
            'å–œæ¬¢': ['åçˆ±', 'å–œæ¬¢', 'çˆ±å¥½', 'é’Ÿæƒ…', 'å¾…è§'],
            'è®¨åŒ': ['åæ„Ÿ', 'è®¨åŒ', 'ä¸å–œæ¬¢', 'åŒæ¶', 'çƒ¦'],
            'å¥½': ['ä¸é”™', 'å¥½', 'æ£’', 'ä¼˜ç§€', 'ç‰›'],
            'å': ['ç³Ÿç³•', 'å', 'å·®åŠ²', 'ä¸è¡Œ', 'ç³Ÿ']
        }
        
        for old, new_list in synonyms.items():
            if old in text and random.random() < 0.3:
                text = text.replace(old, random.choice(new_list))
    
    return text

def adjust_chinese_punctuation(text: str, style: str = 'casual') -> str:
    """è°ƒæ•´ä¸­æ–‡æ ‡ç‚¹"""
    if style in ['casual', 'creative']:
        # æ·»åŠ æ›´å¤šå£è¯­åŒ–æ ‡ç‚¹
        text = text.replace('ï¼›', 'ï¼Œ')
        text = text.replace('ï¼š', 'ï¼š')
        # å¢åŠ æ„Ÿå¹å·ä½¿ç”¨é¢‘ç‡
        sentences = text.split('ã€‚')
        if len(sentences) > 3:
            for i in range(len(sentences)-1):
                if random.random() < 0.2:
                    sentences[i] = sentences[i] + 'ï¼'
        text = 'ã€‚'.join(sentences)
    
    return text

def humanize_chinese_text(text: str, style: str = 'casual', variability: str = 'medium') -> str:
    """å°†ä¸­æ–‡AIæ–‡æœ¬è½¬æ¢ä¸ºæ›´è‡ªç„¶çš„äººç±»é£æ ¼"""
    # 1. æ£€æµ‹AIæ¨¡å¼å¹¶è°ƒæ•´
    patterns = detect_ai_patterns(text)
    
    # å¦‚æœAIç‰¹å¾æ˜æ˜¾ï¼Œå¢åŠ å˜åŒ–ç¨‹åº¦
    if sum(patterns.values()) > 5 and variability == 'low':
        variability = 'medium'
    
    # 2. åŸºç¡€æ›¿æ¢
    text = apply_zh_replacements(text, variability)
    
    # 3. ä¿®å¤å¦å®šå¼æ’æ¯”
    text = fix_negative_parallelism(text)
    
    # 4. å‡å°‘AIè¯æ±‡ä½¿ç”¨
    text = reduce_ai_words(text, variability)
    
    # 5. æ ¹æ®é£æ ¼è°ƒæ•´
    if style == 'casual':
        text = text.replace('ï¼Œ', 'ï¼Œ')
        text = text.replace('ã€‚', 'ã€‚')
    elif style == 'formal':
        pass  # ä¿æŒæ­£å¼é£æ ¼
    elif style == 'creative':
        text = add_chinese_natural_variations(text, variability='high', style='creative')
    
    # 6. è°ƒæ•´å¥å­é•¿åº¦
    text = vary_chinese_sentence_lengths(text, variability)
    
    # 7. å¢åŠ è‡ªç„¶å˜åŒ–
    text = add_chinese_natural_variations(text, variability, style)
    
    # 8. è°ƒæ•´æ ‡ç‚¹
    text = adjust_chinese_punctuation(text, style)
    
    # 9. æœ€ç»ˆæ¸…ç†
    text = re.sub(r'\s+', ' ', text)  # åˆå¹¶å¤šä½™ç©ºæ ¼
    text = re.sub(r'([ã€‚ï¼ï¼Ÿï¼›])\1+', r'\1', text)  # å»é™¤é‡å¤æ ‡ç‚¹
    
    return text.strip()

def main():
    parser = argparse.ArgumentParser(description='AI-Humanizer-ZH - ä¸­æ–‡AIæ–‡æœ¬äººç±»åŒ–å·¥å…·')
    parser.add_argument('text', nargs='?', help='è¦å¤„ç†çš„ä¸­æ–‡æ–‡æœ¬')
    parser.add_argument('--input', help='è¾“å…¥æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output', help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--style', choices=['casual', 'formal', 'creative'], default='casual',
                       help='è¾“å‡ºé£æ ¼: casual/å£è¯­åŒ–, formal/æ­£å¼, creative/åˆ›æ„')
    parser.add_argument('--variability', choices=['low', 'medium', 'high'], default='medium',
                       help='æ–‡æœ¬å˜åŒ–ç¨‹åº¦: low/ä½, medium/ä¸­, high/é«˜')
    parser.add_argument('--preserve', nargs='*', help='éœ€è¦ä¿ç•™çš„å…³é”®è¯æˆ–çŸ­è¯­')
    parser.add_argument('--debug', action='store_true', help='æ˜¾ç¤ºè°ƒè¯•ä¿¡æ¯')
    
    args = parser.parse_args()
    
    # è·å–è¾“å…¥æ–‡æœ¬
    if args.input and os.path.exists(args.input):
        with open(args.input, 'r', encoding='utf-8') as f:
            text = f.read()
    elif args.text:
        text = args.text
    else:
        print('è¯·æä¾›æ–‡æœ¬è¾“å…¥æˆ–æŒ‡å®šè¾“å…¥æ–‡ä»¶')
        return
    
    # æ˜¾ç¤ºè°ƒè¯•ä¿¡æ¯
    if args.debug:
        print('ğŸ“Š æ­£åœ¨æ£€æµ‹AIå†™ä½œæ¨¡å¼...')
        patterns = detect_ai_patterns(text)
        print('ğŸ“‹ AIæ¨¡å¼æ£€æµ‹ç»“æœ:')
        for pattern, count in patterns.items():
            print(f'  {pattern.replace("_", " ").capitalize()}: {count}')
        print()
    
    # ä¿å­˜éœ€è¦ä¿ç•™çš„å†…å®¹
    preserved = {}
    if args.preserve:
        for i, phrase in enumerate(args.preserve):
            placeholder = f'__PRESERVED_{i}__'
            preserved[placeholder] = phrase
            text = text.replace(phrase, placeholder)
    
    # å¤„ç†æ–‡æœ¬
    print(f'âœ… æ­£åœ¨å¤„ç†æ–‡æœ¬ï¼ˆé£æ ¼: {args.style}, å˜åŒ–ç¨‹åº¦: {args.variability}ï¼‰...')
    humanized_text = humanize_chinese_text(text, args.style, args.variability)
    
    # æ¢å¤ä¿ç•™çš„å†…å®¹
    if args.preserve:
        for placeholder, phrase in preserved.items():
            humanized_text = humanized_text.replace(placeholder, phrase)
    
    # è¾“å‡ºç»“æœ
    if args.output:
        with open(args.output, 'w', encoding='utf-8') as f:
            f.write(humanized_text)
        print(f'ğŸ’¾ å¤„ç†åçš„æ–‡æœ¬å·²ä¿å­˜åˆ° {args.output}')
    else:
        print('\nâœ¨ å¤„ç†åçš„æ–‡æœ¬ï¼š')
        print('=' * 60)
        print(humanized_text)
        print('=' * 60)
        
        # æ˜¾ç¤ºä¸€äº›ç»Ÿè®¡ä¿¡æ¯
        original_length = len(text)
        humanized_length = len(humanized_text)
        print(f'\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯ï¼š')
        print(f'  åŸå§‹æ–‡æœ¬é•¿åº¦: {original_length} å­—ç¬¦')
        print(f'  å¤„ç†åé•¿åº¦: {humanized_length} å­—ç¬¦')
        if original_length > 0:
            change_percent = ((humanized_length - original_length) / original_length) * 100
            print(f'  é•¿åº¦å˜åŒ–: {change_percent:+.1f}%')  

if __name__ == '__main__':
    main()
